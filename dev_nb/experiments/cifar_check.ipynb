{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get everything in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.everything import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../../data/cifar10/')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)\n",
    "\n",
    "def ResizeBatch(*size): return Lambda(lambda x: x.view((-1,)+size))\n",
    "def Flatten(): return Lambda(lambda x: x.view((x.size(0), -1)))\n",
    "def PoolFlatten(): return nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2d(ni, nf, ks, stride): return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=False)\n",
    "\n",
    "def bn(ni, init_zero=False):\n",
    "    m = nn.BatchNorm2d(ni)\n",
    "    m.weight.data.fill_(0 if init_zero else 1)\n",
    "    m.bias.data.zero_()\n",
    "    return m\n",
    "\n",
    "def bn_relu_conv(ni, nf, ks, stride, init_zero=False):\n",
    "    bn_initzero = bn(ni, init_zero=init_zero)\n",
    "    return nn.Sequential(bn_initzero, nn.ReLU(inplace=True), conv_2d(ni, nf, ks, stride))\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride, drop_p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(ni)\n",
    "        self.conv1 = conv_2d(ni, nf, 3, stride)\n",
    "        self.conv2 = bn_relu_conv(nf, nf, 3, 1)\n",
    "        self.drop = nn.Dropout(drop_p, inplace=True) if drop_p else None\n",
    "        self.shortcut = conv_2d(ni, nf, 1, stride) if ni != nf else noop\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = F.relu(self.bn(x), inplace=True)\n",
    "        r = self.shortcut(x2)\n",
    "        x = self.conv1(x2)\n",
    "        if self.drop: x = self.drop(x)\n",
    "        x = self.conv2(x) * 0.2\n",
    "        return x.add_(r)\n",
    "\n",
    "\n",
    "def _make_group(N, ni, nf, block, stride, drop_p):\n",
    "    return [block(ni if i == 0 else nf, nf, stride if i == 0 else 1, drop_p) for i in range(N)]\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_groups, N, num_classes, k=1, drop_p=0.0, start_nf=16):\n",
    "        super().__init__()\n",
    "        n_channels = [start_nf]\n",
    "        for i in range(num_groups): n_channels.append(start_nf*(2**i)*k)\n",
    "\n",
    "        layers = [conv_2d(3, n_channels[0], 3, 1)]  # conv1\n",
    "        for i in range(num_groups):\n",
    "            layers += _make_group(N, n_channels[i], n_channels[i+1], BasicBlock, (1 if i==0 else 2), drop_p)\n",
    "\n",
    "        layers += [nn.BatchNorm2d(n_channels[3]), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(1),\n",
    "                   Flatten(), nn.Linear(n_channels[3], num_classes)]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return self.features(x)\n",
    "\n",
    "\n",
    "def wrn_22(): return WideResNet(num_groups=3, N=3, num_classes=10, k=6, drop_p=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrn_22()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way to create datasets in fastai_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset.from_folder(PATH/'train')\n",
    "valid_ds = FilesDataset.from_folder(PATH/'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aug for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_003b import pad_tfm, crop_tfm, flip_lr_tfm, normalize_tfm, apply_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std = map(tensor, ([0.4914, 0.48216, 0.44653], [0.24703, 0.24349, 0.26159]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = [pad_tfm(padding=4), \n",
    "              crop_tfm(size=32, row_pct=(0,1), col_pct=(0,1)), \n",
    "              flip_lr_tfm(p=0.5), \n",
    "              normalize_tfm(mean=data_mean, std=data_std)]\n",
    "valid_tfms = [normalize_tfm(mean=data_mean, std=data_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.everything import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfmDataset(Dataset):\n",
    "    def __init__(self, ds: Dataset, tfms: Collection[Callable] = None, **kwargs):\n",
    "        self.ds,self.tfms,self.kwargs = ds,tfms,kwargs\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getattr__(self, k): return getattr(self.ds, k)\n",
    "#     def fns(self): return self.ds.fns\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if isinstance(idx, tuple): idx,xtra = idx\n",
    "        else: xtra={}\n",
    "        x,y = self.ds[idx]\n",
    "        return apply_tfms(self.tfms)(x, **self.kwargs, **xtra), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tds = TfmDataset(train_ds, train_tfms)\n",
    "valid_tds = TfmDataset(valid_ds, valid_tfms)\n",
    "data = DataBunch(train_tds, valid_tds, bs=512, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(4,4,figsize=(8,8))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    img = train_tds[0][0] * data_std[:,None,None] + data_mean[:,None,None]\n",
    "    ax.imshow(img.numpy().transpose(1,2,0))\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learner wraps together the data and the model, like in fastai. Here we test the usual training to 94% accuracy with AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrn_22()\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.true_wd = True\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, 3e-3, 1, wd=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same but in mixed-precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrn_22()\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.true_wd = True\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "to_fp16(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fit_one_cycle(learn, 3e-3, 30, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
