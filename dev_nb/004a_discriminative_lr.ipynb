{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_004 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm,cifar_denorm = normalize_funcs(data_mean,data_std)\n",
    "\n",
    "train_tfms = [flip_lr_tfm(p=0.5),\n",
    "              pad_tfm(padding=4),\n",
    "              crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.))]\n",
    "valid_tfms = []\n",
    "\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaks to the OptimWrapper to handle an array of lrs/wds/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OptimWrapper():\n",
    "    \"Basic wrapper around an optimizer to simplify HP changes\"\n",
    "    def __init__(self, opt:optim.Optimizer, wd:Floats=0., true_wd:bool=False):\n",
    "        self.opt,self.true_wd = opt,true_wd\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "        self._wd = self.listify(wd, self.opt.param_groups)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'OptimWrapper over {repr(self.opt)}.\\nTrue weight decay: {self.true_wd}'\n",
    "\n",
    "    #Pytorch optimizer methods\n",
    "    def step(self):\n",
    "        # weight decay outside of optimizer step (AdamW)\n",
    "        if self.true_wd:\n",
    "            for lr,wd,pg in zip(self._lr,self._wd,self.opt.param_groups):\n",
    "                for p in pg['params']: p.data.mul_(1 - wd*lr)\n",
    "            self.set_val('weight_decay', 0)\n",
    "        self.opt.step()\n",
    "    \n",
    "    def zero_grad(self): self.opt.zero_grad()\n",
    "    \n",
    "    #Hyperparameters as properties\n",
    "    @property\n",
    "    def lr(self) -> float: return self._lr[-1]\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, val:float): self._lr = self.set_val('lr', self.listify(val, self._lr))\n",
    "    \n",
    "    @property\n",
    "    def mom(self) -> float: return self._mom[-1]\n",
    "\n",
    "    @mom.setter\n",
    "    def mom(self, val:float):\n",
    "        if 'momentum' in self.opt_keys: self.set_val('momentum', self.listify(val, self._mom))\n",
    "        elif 'betas' in self.opt_keys:  self.set_val('betas', (self.listify(val, self._mom), self._beta))\n",
    "        self._mom = self.listify(val, self._mom)\n",
    "    \n",
    "    @property\n",
    "    def beta(self) -> float: return None if self._beta is None else self._beta[-1]\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val:float):\n",
    "        if val is None: return\n",
    "        if 'betas' in self.opt_keys:    self.set_val('betas', (self._mom, self.listify(val, self._beta)))\n",
    "        elif 'alpha' in self.opt_keys:  self.set_val('alpha', self.listify(val, self._beta))\n",
    "        self._beta = self.listify(val, self._beta)\n",
    "    \n",
    "    @property\n",
    "    def wd(self) -> float: return self._wd[-1]\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val:float):\n",
    "        if not self.true_wd: self.set_val('weight_decay', self.listify(val, self._wd))\n",
    "        self._wd = self.listify(val, self._wd)\n",
    "    \n",
    "    #Helper functions\n",
    "    def read_defaults(self):\n",
    "        \"Read the values inside the optimizer for the hyper-parameters\"\n",
    "        self._beta = None\n",
    "        if 'lr' in self.opt_keys: self._lr = self.read_val('lr')\n",
    "        if 'momentum' in self.opt_keys: self._mom = self.read_val('momentum')\n",
    "        if 'alpha' in self.opt_keys: self._beta = self.read_val('alpha')\n",
    "        if 'betas' in self.opt_keys: self._mom,self._beta = self.read_val('betas')\n",
    "        if 'weight_decay' in self.opt_keys: self._wd = self.read_val('weight_decay')\n",
    "    \n",
    "    def set_val(self, key:str, val):\n",
    "        \"Set the values inside the optimizer dictionary at the key\"\n",
    "        if is_tuple(val): val = [(v1,v2) for v1,v2 in zip(*val)]\n",
    "        for v,pg in zip(val,self.opt.param_groups): pg[key] = v\n",
    "        return val\n",
    "    \n",
    "    def read_val(self, key:str) -> Union[List[float],Tuple[List[float],List[float]]]:\n",
    "        \"Read a hyper-parameter key in the optimizer dictionary.\"\n",
    "        val = [pg[key] for pg in self.opt.param_groups]\n",
    "        if is_tuple(val[0]): val = [o[0] for o in val], [o[1] for o in val]\n",
    "        return val\n",
    "    \n",
    "    def listify(self, p, q) -> List[Any]:\n",
    "        \"Wrap listify with an assert.\"\n",
    "        if is_listy(p): assert len(p) == len(q), f'Passing {len(p)} hyperparameters when we have {len(q)} groups.'\n",
    "        return listify(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_model(model:nn.Module, idx:Sequence[int]) -> List[nn.Module]:\n",
    "    \"Split the model according to the layers index in idx\"\n",
    "    layers = list(model.children())\n",
    "    if idx[0] != 0: idx = [0] + idx\n",
    "    if idx[-1] != len(layers): idx.append(len(layers))\n",
    "    return [nn.Sequential(*layers[i:j]) for i,j in zip(idx[:-1],idx[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model(model, [5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_groups = split_model(model, [5,9])\n",
    "layer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-3,1e-2,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_opt = OptimWrapper(optim.SGD([{'params':l.parameters(), 'lr':lr} for l,lr in zip(layer_groups, lrs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_opt.lr, tst_opt._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_opt.wd, tst_opt._wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_opt.wd = [0.1,0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's tweak the learner to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Learner():\n",
    "    \"Object that wraps together some data, a model, a loss function and an optimizer\"\n",
    "    \n",
    "    data:DataBunch\n",
    "    model:nn.Module\n",
    "    opt_fn:Callable=optim.SGD\n",
    "    loss_fn:Callable=F.cross_entropy\n",
    "    metrics:Collection[Callable]=None\n",
    "    true_wd:bool=False\n",
    "    layer_groups:Collection[nn.Module]=None\n",
    "    def __post_init__(self): \n",
    "        self.model = self.model.to(self.data.device)\n",
    "        self.callbacks = []\n",
    "\n",
    "    def fit(self, epochs:int, lr:Floats, wd:Floats=0., callbacks:Collection[Callback]=None):\n",
    "        if not hasattr(self, 'opt'): self.create_opt(lr, wd)\n",
    "        else: self.opt.wd = wd\n",
    "        if callbacks is None: callbacks = []\n",
    "        callbacks = self.callbacks + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics)\n",
    "    \n",
    "    def create_opt(self, lr:Floats, wd:Floats=0.):\n",
    "        if self.layer_groups is None: self.layer_groups = [self.model]\n",
    "        lrs = listify(lr, self.layer_groups)\n",
    "        opt = self.opt_fn([{'params':l.parameters(), 'lr':lr} for l,lr in zip(self.layer_groups, lrs)])\n",
    "        self.opt = OptimWrapper(opt, wd=wd, true_wd=self.true_wd)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        self.callbacks = [self.recorder] + self.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset.from_folder(PATH/'train', classes=['airplane','dog'])\n",
    "valid_ds = FilesDataset.from_folder(PATH/'test', classes=['airplane','dog'])\n",
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=train_tfms, valid_tfm=valid_tfms, num_workers=4, dl_tfms=cifar_norm)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "layer_groups = split_model(model.layers, [5,9])\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.layer_groups = layer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.layer_groups = split_model(model.layers, [5,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lrs, wd=[1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt._wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lrs, wd=[1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LRs and WDs are the easiest to pass through the Learner, but if a Callback sets an array of moms or betas, the OptimWrapper will handle them as discriminative moms/betas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how it fits with the other callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn, start_lr=lrs/1000, end_lr=lrs*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = OneCycleScheduler(learn, lrs, 1)\n",
    "learn.fit(1, lrs, callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
