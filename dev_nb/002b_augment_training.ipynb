{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_002 import *\n",
    "\n",
    "import typing\n",
    "from typing import Dict, Any, AnyStr, List, Sequence, TypeVar, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "# PATH = DATA_PATH/'cifar10_dog_air'\n",
    "PATH = DATA_PATH/'cifar10'\n",
    "\n",
    "train_ds = FilesDataset(PATH/'train')\n",
    "valid_ds = FilesDataset(PATH/'test', train_ds.classes)\n",
    "\n",
    "x = train_ds[1][0]\n",
    "bs=256\n",
    "c = len(train_ds.classes)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crop**\n",
    "\n",
    "Crop is a transform that cuts a series of pixels from an image. It does this by removing rows and columns from the input image.\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Size** Size of our final image in pixels.\n",
    "\n",
    "    Domain: Positive integers.\n",
    "    \n",
    "2. **Row_pct** Determines where to cut our image vertically on the bottom and top (which rows are left out). If <0.5, more rows will be cut in the top than in the bottom and viceversa (varies linearly).\n",
    "\n",
    "    Domain: Real numbers between 0 and 1.\n",
    "    \n",
    "3. **Col_pct** Determines where to cut our image horizontally on the left and right (which columns are left out). If <0.5, more rows will be cut in the left than in the right and viceversa (varies linearly).\n",
    "\n",
    "    Domain: Real numbers between 0 and 1.\n",
    "    \n",
    "Our three parameters are related with the following equations:\n",
    "\n",
    "1. output_rows = [**row_pct***(input_rows-**size**):**size**+**row_pct***(input_rows-**size**)]\n",
    "\n",
    "2. output_cols = [**col_pct***(input_cols-**size**):**size**+**col_pct***(input_cols-**size**)]\n",
    "\n",
    "**Pad**\n",
    "\n",
    "\n",
    "Pads each of the four borders of our image with a certain amount of pixels. Can pad with reflection (reflects border pixels to fill new pixels) or zero (adds black pixels). \n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Padding** Amount of pixels to add to each border. [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad)\n",
    "\n",
    "    Domain: Positive integers.\n",
    "    \n",
    "2. **Mode** How to fill new pixels. For more detail see the Pytorch subfunctions for padding.\n",
    "\n",
    "    Domain: \n",
    "    - Reflect (default): reflects opposite pixels to fill new pixels. [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.ReflectionPad2d)\n",
    "    - Constant: adds pixels with specified value (default is 0, black pixels) [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.ConstantPad2d)\n",
    "    - Replicate: replicates border row or column pixels to fill new pixels [More details](https://pytorch.org/docs/stable/nn.html#torch.nn.ReplicationPad2d)\n",
    "    \n",
    "    \n",
    "***On using padding and crop***\n",
    "\n",
    "A nice way to use these two functions is to combine them into one transform. We can add padding to the image and then crop some of it out. This way, we can create a new image to augment our training set without losing image information by cropping. Furthermore, this can be done in several ways (modifying the amount and type of padding and the crop style) so it gives us great flexibility to add images to our training set. You can find an example of this in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@reg_transform\n",
    "def pad(x, padding, mode='reflect') -> TfmType.Start:\n",
    "    return F.pad(x[None], (padding,)*4, mode=mode)[0]\n",
    "\n",
    "@reg_transform\n",
    "def crop(x, size, row_pct:uniform=0.5, col_pct:uniform=0.5) -> TfmType.Pixel:\n",
    "    size = listify(size,2)\n",
    "    rows,cols = size\n",
    "    row = int((x.size(1)-rows+1) * row_pct)\n",
    "    col = int((x.size(2)-cols+1) * col_pct)\n",
    "    return x[:, row:row+rows, col:col+cols].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(crop(pad(x, 4, 'constant'), 32, 0.25, 0.75), hide_axis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(crop(pad(x, 4), 32, 0.25, 0.75), hide_axis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5),\n",
    "        pad_tfm(padding=4),\n",
    "        crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TfmDataset(Dataset):\n",
    "    def __init__(self, ds: Dataset, tfms: Collection[Callable] = None, **kwargs):\n",
    "        self.ds,self.tfms,self.kwargs = ds,tfms,kwargs\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.ds[idx]\n",
    "        if self.tfms is not None: x = apply_tfms(self.tfms)(x, **self.kwargs)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tds = TfmDataset(train_ds, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,9))\n",
    "for ax in axes.flat: show_image(train_tds[1][0], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our network the first step is to normalize our pixels. This makes our cost function faster and easier to optimize [(see Yann le Cun's paper, section 4.3)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "Normalization is a pixel transform since it directly modifies the pixels of our input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first train a simple CNN with our normalized and transformed data to make sure that the training goes smoothly. Please note that we do not transform our validation data (except for normalization). This is because we are using the transforms to generate synthetic data to improve our training (help our model generalize better) but, when assessing performance in the validation set, we do not need extra data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize(x, mean,std):   return (x-mean[...,None,None]) / std[...,None,None]\n",
    "def denormalize(x, mean,std): return x*std[...,None,None] + mean[...,None,None]\n",
    "\n",
    "def normalize_batch(b, mean, std, do_y=False):\n",
    "    x,y = b\n",
    "    x = normalize(x,mean,std)\n",
    "    if do_y: y = normalize(y,mean,std)\n",
    "    return x,y\n",
    "\n",
    "def normalize_funcs(mean, std, do_y=False):\n",
    "    return (partial(normalize_batch, mean=mean.to(default_device),std=std.to(default_device)),\n",
    "            partial(denormalize,     mean=mean,                   std=std))\n",
    "\n",
    "@dataclass\n",
    "class DeviceDataLoader():\n",
    "    dl: DataLoader\n",
    "    device: torch.device\n",
    "    progress_func:Callable=None\n",
    "    tfms: List[Callable]=None\n",
    "\n",
    "    def __len__(self): return len(self.dl)\n",
    "\n",
    "    def proc_batch(self,b):\n",
    "        b = to_device(self.device,b)\n",
    "        return b if self.tfms is None else self.tfms(b)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.gen = map(self.proc_batch, self.dl)\n",
    "        if self.progress_func is not None:\n",
    "            self.gen = self.progress_func(self.gen, total=len(self.dl), leave=False)\n",
    "        return iter(self.gen)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *args, device=default_device, progress_func=tqdm, tfms=tfms, **kwargs):\n",
    "        return cls(DataLoader(*args, **kwargs), device=device, progress_func=progress_func, tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 stats looked up on google \n",
    "cifar_mean,cifar_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm,cifar_denorm = normalize_funcs(cifar_mean,cifar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader.create(train_ds, bs, tfms=cifar_norm, shuffle=True, progress_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))\n",
    "x = x.cpu()\n",
    "print(x.min(),x.max(),x.mean(),x.std())\n",
    "x = cifar_denorm(x)\n",
    "show_images(x,y,6,train_ds.classes, figsize=(9,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_ds, valid_ds, bs=64, device=None, num_workers=4, **kwargs):\n",
    "        self.device = default_device if device is None else device\n",
    "        self.train_dl = DeviceDataLoader.create(train_ds, bs,   shuffle=True,  num_workers=num_workers, **kwargs)\n",
    "        self.valid_dl = DeviceDataLoader.create(valid_ds, bs*2, shuffle=False, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, train_tfm=None, valid_tfm=None, dl_tfms=None, **kwargs):\n",
    "        return cls(TfmDataset(train_ds, train_tfm), TfmDataset(valid_ds, valid_tfm), tfms=dl_tfms, **kwargs)\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=tfms, dl_tfms=cifar_norm, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))\n",
    "x = x.cpu()\n",
    "print(x.min(),x.max(),x.mean(),x.std())\n",
    "x = cifar_denorm(x)\n",
    "show_images(x,y,6,train_ds.classes, figsize=(9,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, simple_cnn([3,16,16,c], [3,3,3], [2,2,2]))\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, 0.1, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to try our transforms on an architecture similar to the [darknet-53](https://pjreddie.com/media/files/papers/yolo.pdf) architecture. Note that it is not the whole architecture, just the part of it that the authors pre-trained on Imagenet (see paper, section 2.2). This is the basis of any modern ResNet based architecture and it is good for experimenting.\n",
    "\n",
    "If you are interested in a full, step-by-step description of this architecture please refer to a [video explanation](https://youtu.be/ondivPiwQho?t=0h11m07s) in Lesson 12 of Part 2 of the course or a [written transcript](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94), courtesy of @hiromi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_layer(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_layer(ni//2, ni, ks=3)\n",
    "        \n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def make_group_layer(self, ch_in, num_blocks, stride=1):\n",
    "        return [conv_layer(ch_in, ch_in*2,stride=stride)\n",
    "               ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes, nf=32):\n",
    "        super().__init__()\n",
    "        layers = [conv_layer(3, nf, ks=3, stride=1)]\n",
    "        for i,nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, num_classes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 4, 2], num_classes=c, nf=16)\n",
    "# model = Darknet([1, 2, 4, 6, 3], num_classes=c, nf=32)\n",
    "learner = Learner(data, model)\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1, 0.1, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lr in (0.1,0.2,0.4,0.8,0.1,0.01):\n",
    "#     momentum = 0.95 if lr<0.1 else 0.85 if lr>0.5 else 0.9\n",
    "#     learner.fit(2, lr, opt_fn=partial(optim.SGD, momentum=momentum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
